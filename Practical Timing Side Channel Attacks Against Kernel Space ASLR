Talks about control-flow hijacking. Defense mechanism developed for hijacking. Some of mechanism include:

- Stack canaries
- Non-executable memory
- ASLR (Addres Space Layout Randomization)

In this paper, limitations of kernel space ASLR against a *local* attacker with restricted privileges is studied. It is shown in the paper that it is possible for an local adversary to implement a generic side channel attack against the memory management system to deduce info about about the privileged address space layout.

--> Q) What is a side channel?


Their approach is based on the property that the different caches are *shared* resources on computers.

Key idea behind ASLR is that it randomizes system's virtual memory layout either every time a new code execution starts (e.g., upon process creation or when a driver is loaded.) or on each reboot. Initial purpose was to randomize user mode processes. Modern OSes randomize *both* user and kernel space. Thanks to ASLR, advanced exploitation techniques like return-to-libc and return-oriented programming (ROP) are prevented since an adversary does not know virtual address of memory locations to which he can divert control flow.

An attacker might attempt to perform a brute-force attack on ASLR systems. User mode ASLR on 32-bit architectures only leaves for 16 bit of randomness, which is not enough to defeat brute-force attacks. However, same attacks on kernel mode ASLR are not practical on 32-bit architectures: If an attacker wants to exploit a vulnerability in kernel code, a wrong offset will typically lead to system crash and thus attacker has only 1 attempt to perform an exploit, which is not feasible in practice, so we are fine.

Together with DEP, a technique that enforces Writable XOR Executable property of memory pages, ASLR greatly reduces the attack surface.

***
Sidenote: Data Execution Prevention (DEP) is a system-level memory protection feature that is built into the operating system. DEP enables the system to mark one or more pages of memory as non-executable.
***

This paper introduces a generic attack for systems running on the Intel ISA. More specifically, the paper shows how a local attacker with restricted rights can mount a timing-based side channel attack against the memory management system to deduce info about the privileged address space layout. They took advantage of the fact that the memory hierarchy present in computers lead to shared resources between user and kernel space code that can be abused to construct a side channel. In practice, timing attacks against a modern CPU are very complicated due to the many performance optimizations used by current processors such as HW prefetching, speculative execution, multi-core architectures, or branch prediction that significantly complicate timing measurements.

--> Q) What is speculative execution?

In summary, contributions of this papers are the following:

1) An attack to derandomize kernel space ASLR that relies on a side channel based on the memory hierarchy present in computers, which leads to timing differences when accessing specific memory regions.
2) Three different approaches are presented to implement the attack. 
***As part of the implementation, an undocumented hash function used in Intel Sandybridge CPUs is reverse-engineered to distribute the cache among different cores.***
3) Several mitigation strategies that defeats the presented attack are discussed.

===
Technical background: 

ASLR:

It randomizes the *base* address of important memory structures such as code, stack and heap. Consequently, an adversary does not know the virtual address of relevant memory locations needed to perform a control-flow hijacking attack (i.e., location of shellcode or ROP gadgets).

--> Q) What are shellcode and ROP gadgets?

Note that since a user mode app has no means to *directly* access the kernel space, it cannot determine the base addresses the kernel modules are loaded to: every attempt to access kernel space memory from user mode results in an access violation, and thus kernel space ASLR effectively hampers local exploits against the OS kernel or drivers.


Memory Hierarchy:

Modern hierarchies range from *a few* very fast CPU registers over different levels of cache to a huge and rather slow main memory (RAM). Paper focuses on different caches that are used to speed up address translation and memory accesses for code and data. Typically, each CPU core containes 1 dedicated L1 and L2 cache and often there is an additional shared L3 cache (also called last level cache, LLC). L1 cache is split into data and instruction caches named ICACHE and DCACHE. On higher stages, unified caches are used, meaning that data and cache is not separated from each other. The efficieny of cache usage is justified by the temporal and spatial locality property of memory accesses. Hence, not only single bytes of are cached, but always a chunk of adjacent memory.

All described caches operate in an n-way set associative mode. Here, all available slots are grouped into sets of size n and each memory chunk can be stored in all slots of one *particular* set. This target set is determined by a bunch of cache index bits that are taken from the memory address.

As an example, consider 32-bit address and a typical L3 cache of 8 MB that is 16-way set associative. 
Remember that cache line is of size 64 *bytes* on most systems.

8MB = 8*1024*1024 = 8388608 bytes. 
8388608/64 = 131072 slots.

There are 131072 single slots that are *grouped into* 131072/16=8192 different sets. Hence, log2(8192)=13 bits from a virtual address are needed to select the appropriate set. Since the lower 6 bits of each address are used to select on particular bytes from each cache line (log2(64)=6), the bits 18 to 6 determine the set. The remaining upper 13 bits form the *address tag*, that has to be *stored with each cache line* for the later lookup. (6+13+13=32 bits in total to represent a virtual address in 32-bit system.)

One important consequence of set associativity is that memory addresses with *identical* index bits compete against the available slots of one set (Addresses do not have any specific slot!). Hence, memory accesses may evict and replace other memory contents from the caches.  A common replacement strategy is "Least Recently Used (LRU)", in which the entry which has not been accessed for the longest time is replaced. Since the managing real timestamps is not affordable in practice, the variant "Pseudo-LRU" is used: an additional reference bit is stored with each cache line that is set on each access. 

****Once all reference bits of a set are enabled, they are all cleared again. If an entry from a set has to be removed, an arbitrary one with a cleared reference bit is chosen.****


Virtual Memory and Address Translation:

Modern OSes usually work on *paged virtual memory* instead of physical memory. The memory space is divided into equally sized pages that are either regular pages (e.g., with a size of 4 KB), or large pages (e.g., 2 or 4 MB (btw, this is probably not true nowadays)). When accesssing memory via virtual addresses (VA), they first have to be translated into physical addresses (PA) by the processor's "Memory Management Unit (MMU)" in a *page walk*: the *virtual* address is split into several parts and each part operates as an array index for certain levels of *page tables.* The lowest level of the involved *paging structures (PS)*, the *Page Table Entry (PTE)* contains the resulting physical *frame number.* For large pages, 1 level less of PS is needed since a larger space of memory requires less bits to address.

===

