Talks about control-flow hijacking. Defense mechanism developed for hijacking. Some of mechanism include:

- Stack canaries
- Non-executable memory
- ASLR (Addres Space Layout Randomization)

In this paper, limitations of kernel space ASLR against a *local* attacker with restricted privileges is studied. It is shown in the paper that it is possible for an local adversary to implement a generic side channel attack against the memory management system to deduce info about about the privileged address space layout.

--> Q) What is a side channel?


Their approach is based on the property that the different caches are *shared* resources on computers.

Key idea behind ASLR is that it randomizes system's virtual memory layout either every time a new code execution starts (e.g., upon process creation or when a driver is loaded.) or on each reboot. Initial purpose was to randomize user mode processes. Modern OSes randomize *both* user and kernel space. Thanks to ASLR, advanced exploitation techniques like return-to-libc and return-oriented programming (ROP) are prevented since an adversary does not know virtual address of memory locations to which he can divert control flow.

An attacker might attempt to perform a brute-force attack on ASLR systems. User mode ASLR on 32-bit architectures only leaves for 16 bit of randomness, which is not enough to defeat brute-force attacks. However, same attacks on kernel mode ASLR are not practical on 32-bit architectures: If an attacker wants to exploit a vulnerability in kernel code, a wrong offset will typically lead to system crash and thus attacker has only 1 attempt to perform an exploit, which is not feasible in practice, so we are fine.

Together with DEP, a technique that enforces Writable XOR Executable property of memory pages, ASLR greatly reduces the attack surface.

***
Sidenote: Data Execution Prevention (DEP) is a system-level memory protection feature that is built into the operating system. DEP enables the system to mark one or more pages of memory as non-executable.
***

This paper introduces a generic attack for systems running on the Intel ISA. More specifically, the paper shows how a local attacker with restricted rights can mount a timing-based side channel attack against the memory management system to deduce info about the privileged address space layout. They took advantage of the fact that the memory hierarchy present in computers lead to shared resources between user and kernel space code that can be abused to construct a side channel. In practice, timing attacks against a modern CPU are very complicated due to the many performance optimizations used by current processors such as HW prefetching, speculative execution, multi-core architectures, or branch prediction that significantly complicate timing measurements.

--> Q) What is speculative execution?

In summary, contributions of this papers are the following:

1) An attack to derandomize kernel space ASLR that relies on a side channel based on the memory hierarchy present in computers, which leads to timing differences when accessing specific memory regions.
2) Three different approaches are presented to implement the attack. 
***As part of the implementation, an undocumented hash function used in Intel Sandybridge CPUs is reverse-engineered to distribute the cache among different cores.***
3) Several mitigation strategies that defeats the presented attack are discussed.

===
Technical background: 

ASLR:

It randomizes the *base* address of important memory structures such as code, stack and heap. Consequently, an adversary does not know the virtual address of relevant memory locations needed to perform a control-flow hijacking attack (i.e., location of shellcode or ROP gadgets).

--> Q) What are shellcode and ROP gadgets?

Note that since a user mode app has no means to *directly* access the kernel space, it cannot determine the base addresses the kernel modules are loaded to: every attempt to access kernel space memory from user mode results in an access violation, and thus kernel space ASLR effectively hampers local exploits against the OS kernel or drivers.


Memory Hierarchy:

Modern hierarchies range from *a few* very fast CPU registers over different levels of cache to a huge and rather slow main memory (RAM). Paper focuses on different caches that are used to speed up address translation and memory accesses for code and data. Typically, each CPU core containes 1 dedicated L1 and L2 cache and often there is an additional shared L3 cache (also called last level cache, LLC). L1 cache is split into data and instruction caches named ICACHE and DCACHE. On higher stages, unified caches are used, meaning that data and cache is not separated from each other. The efficieny of cache usage is justified by the temporal and spatial locality property of memory accesses. Hence, not only single bytes of are cached, but always a chunk of adjacent memory.

All described caches operate in an n-way set associative mode. Here, all available slots are grouped into sets of size n and each memory chunk can be stored in all slots of one *particular* set. This target set is determined by a bunch of cache index bits that are taken from the memory address.

As an example, consider 32-bit address and a typical L3 cache of 8 MB that is 16-way set associative. 
Remember that cache line is of size 64 *bytes* on most systems.

8MB = 8*1024*1024 = 8388608 bytes. 
8388608/64 = 131072 slots.

There are 131072 single slots that are *grouped into* 131072/16=8192 different sets. Hence, log2(8192)=13 bits from a virtual address are needed to select the appropriate set. Since the lower 6 bits of each address are used to select on particular bytes from each cache line (log2(64)=6), the bits 18 to 6 determine the set. The remaining upper 13 bits form the *address tag*, that has to be *stored with each cache line* for the later lookup. (6+13+13=32 bits in total to represent a virtual address in 32-bit system.)

One important consequence of set associativity is that memory addresses with *identical* index bits compete against the available slots of one set (Addresses do not have any specific slot!). Hence, memory accesses may evict and replace other memory contents from the caches.  A common replacement strategy is "Least Recently Used (LRU)", in which the entry which has not been accessed for the longest time is replaced. Since the managing real timestamps is not affordable in practice, the variant "Pseudo-LRU" is used: an additional reference bit is stored with each cache line that is set on each access. 

****Once all reference bits of a set are enabled, they are all cleared again. If an entry from a set has to be removed, an arbitrary one with a cleared reference bit is chosen.****


Virtual Memory and Address Translation:

Modern OSes usually work on *paged virtual memory* instead of physical memory. The memory space is divided into equally sized pages that are either regular pages (e.g., with a size of 4 KB), or large pages (e.g., 2 or 4 MB (btw, this is probably not true nowadays)). When accesssing memory via virtual addresses (VA), they first have to be translated into physical addresses (PA) by the processor's "Memory Management Unit (MMU)" in a *page walk*: the *virtual* address is split into several parts and each part operates as an array index for certain levels of *page tables.* The lowest level of the involved *paging structures (PS)*, the *Page Table Entry (PTE)* contains the resulting physical *frame number.* For large pages, 1 level less of PS is needed since a larger space of memory requires less bits to address. PAPERS FURTHER EXPLAINS DETAILS HERE.

In order to speed up this address translation process, resolved address mappings are cached in Translation Lookaside Buffers (TLBs). Additionally there often are dedicated caches for the involved higher level Paging Structure (PS). Current x86/x64 systems usually have two different levels of TLB: the first stage TLB0 is split into one for data (DTLB) and another for instructions (ITLB), and second stage TLB1 is used for both. 

TLB --> TLB0 --> DTLB
             --> ITLB 
             
    --> TLB1 --> Unified TLB used both for data and instructions.



Virtual address translation related caches --> TLB
                                           --> PS cache



Even with TLB and PS caches, the address translation takes some clock cycles, during which the resulting physical address is not available yet. As an effect, the system has to wait for the address translation *before* it can check the tag values of the caches. Therefore, lower caches are ***virtually indexed but physically tagged. This means that the cache index is taken from the virtual address but the stored tag values are taken from the physical address.*** 

Note that PS are not only cached in the TLB or PML4/PDP/PDE caches, but may also reside as regular data within the DCACHE or higher level *unified* caches.

An essential part of virtual memory system is the page fault handler (PFH). It is invoked if a virtual address cannot be resolved, i.e., the page walk encounters invalid PS. This may happen for several reasons: addressed memory region may been swapped out to disk, ***or the memory is accessed for the first time after its allocation. Although page fault happens transparently, the process induces a slight time delay. Besides translation info, PS also contains several protection flags. These flags can mark memory as non-executable or can restrict access to privileged code only. After a successful translation, these flags are checked against the current system state and in case of protection violation, the PFH is invoked.

Entire virtual address space is divided in both user and kernel space. So, attacker might attempt to directly jump to a user space address from within kernel mode in an exploit, thus circumventing any kernel space ASLR protections. However, this is not always possible since the correct user space might not be mapped at the time of the exploit. Furthermore, this kind of attack is rendered impossible with the introduction of the Supervisor Mode Execution Protection (SMEP) feature of modern CPUs ***that disables execution of user space addresses in kernel mode.***

Paper shows that even in the absence of any kernel information leak, they can still de-randomize kernel space ASLR.

Q) --> What is "enforcing W XOR R" property?

Operating system's system call handler is present in CPU caches while executing a syscall.

An attacker can (to a certain degree) control which code or data regions are accessed in kernel mode **by forcing fixed execution paths and known data access patterns in the kernel.** For example, user mode code can perform a system call (sysenter, sysenter is a i586 instruction, specifically tight to 32-bits applications. It has been subsumed by syscall on 64-bits plateforms. --> https://reverseengineering.stackexchange.com/questions/2869/how-to-use-sysenter-under-linux) or an interrupt. 

***This will force the CPU to cache the associated handler code and data structures (e.g., IDT table, Interrupt Descriptor Table --> The Interrupt Descriptor Table (IDT) is a data structure used by the x86 architecture to implement an interrupt vector table. The IDT is used by the processor to determine the correct response to interrupts and exceptions.) as well as data accessed by the handler code (e.g., system call table).
===

Side channels emerge from intricacies of the underlying HW and the fact that parts of the HW like caches and physical memory are shared between both privilaged and non-privileged code.
